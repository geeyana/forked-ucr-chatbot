import os
import google.generativeai as genai
import ollama
from typing import Generator, Optional, List

# --- Configuration ---
# Use environment variables to set the mode and API key.

API_KEY = os.environ.get("GEMINI_API_KEY")
MODE = os.environ.get("LLM_MODE", "testing") # Defaults to testing mode

# This global client will be instantiated in main() as either a Gemini or Ollama client.
client = None

def main():
    """Initializes the global client based on the current mode."""
    global client
    if MODE == "production":
        client = Gemini(API_KEY)
        print("Running in Production Mode (Gemini)")
    else:
        client = Ollama()
        print("Running in Testing Mode (Ollama)")

def get_response_from_prompt(prompt: str, max_tokens: int = 2048) -> str:
    """Gets a response by calling the currently configured global client.

    :param prompt: The prompt to feed into the language model.
    :param max_tokens: The maximal number of tokens generated by the language model.
    :return: The completion from the language model.
    """
    return client.get_response(prompt, max_tokens)


def stream_response_from_prompt(
    prompt: str, max_tokens: int = 2048
) -> Generator[str, None, None]:
    """Streams a response by calling the currently configured global client.

    :param prompt: The prompt to feed into the language model.
    :param max_tokens: The maximal number of tokens generated by the language model.
    :yield: The next substring of the language model's completion.
    """
    yield from client.stream_response(prompt, max_tokens)

# --- Client Classes ---

class Gemini:
    """A class representation of the Gemini 1.5 Pro API."""
    def __init__(self, key: str):
        if not key:
            raise ValueError("A Gemini API key is required for production mode.")
        genai.configure(api_key=key)
        self.model = genai.GenerativeModel(model_name="gemini-1.5-pro")
        self.temp = 1.0
        self.stop_sequences = []

    def get_response(self, prompt: str, max_tokens: int = 2048) -> str:
        """Gets a response from the Gemini model."""
        config = {
            "temperature": self.temp,
            "max_output_tokens": max_tokens,
            "stop_sequences": self.stop_sequences,
        }
        response = self.model.generate_content(prompt, generation_config=config)
        return response.text

    def stream_response(self, prompt: str, max_tokens: int = 2048) -> Generator[str, None, None]:
        """Streams a response from the Gemini model."""
        config = {
            "temperature": self.temp,
            "max_output_tokens": max_tokens,
            "stop_sequences": self.stop_sequences,
        }
        response = self.model.generate_content(prompt, generation_config=config, stream=True)
        for part in response:
            yield part.text

    def set_temp(self, temp: float) -> None:
        """Sets the generation temperature for the model."""
        if not (0.0 <= temp <= 2.0):
            raise ValueError("Temperature must be between 0.0 and 2.0.")
        self.temp = temp

    def set_stop_sequences(self, stop: List[str]) -> None:
        """Sets the stop sequences for the model."""
        if not isinstance(stop, list):
            raise TypeError("Stop sequences must be a list of strings.")
        if len(stop) > 5:
            raise ValueError("The list of stop sequences cannot contain more than 5 items.")
        self.stop_sequences = stop


class Ollama:
    """A class representation for a local Ollama API."""
    def __init__(self, model: str = "gemma:2b", host: str = "http://localhost:11434"):
        self.model = model
        self.temp = 0.7
        self.stop_sequences = None
        try:
            self.client = ollama.Client(host=host)
            self.client.list()
        except Exception:
            raise ConnectionError(f"Could not connect to Ollama at {host}. Please ensure Ollama is running.")

    def get_response(self, prompt: str, max_tokens: int = 2048) -> str:
        """Gets a single response from the Ollama model."""
        options = {"temperature": self.temp, "num_predict": max_tokens, "stop": self.stop_sequences}
        response = self.client.generate(model=self.model, prompt=prompt, stream=False, options=options)
        return response.get('response', '')

    def stream_response(self, prompt: str, max_tokens: int = 2048) -> Generator[str, None, None]:
        """Streams a response from the Ollama model."""
        options = {"temperature": self.temp, "num_predict": max_tokens, "stop": self.stop_sequences}
        stream = self.client.generate(model=self.model, prompt=prompt, stream=True, options=options)
        for chunk in stream:
            yield chunk.get('response', '')

    def set_temp(self, temp: float) -> None:
        """Sets the generation temperature for the model."""
        self.temp = temp

    def set_stop_sequences(self, stop: List[str]) -> None:
        """Sets the stop sequences for the model."""
        if not isinstance(stop, list):
            raise TypeError("Stop sequences must be a list of strings.")
        self.stop_sequences = stop


if __name__ == "__main__":
    main()