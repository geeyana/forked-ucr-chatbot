"""
First, complete the skeleton functions for the language model API that exist in the 
repository by adding in logic that calls some language model API. Add any useful 
keyword arguments to the functions that you see fit, like temperature, max_tokens, 
or stop_sequences. You should make two versions: one version for production that will 
use either the Gemini API or another provider permitted by campus IT; and another version 
for testing that uses Ollama, which is a local hosting tool for language models. For Ollama,
use a small model like Gemma3n or the 1.5 Billion parameter version of Deepseek, that it
may work on a development machine without a GPU.
"""
# make functions that call the class to make get response and stream response
# take a look at api/routes.py

import google.generativeai as genai
from typing import Generator

API_KEY = # type key here (str)

def main():
    client = Gemini(API_KEY)

    
def get_response_from_prompt(prompt: str, max_tokens: int = 3000) -> str:
    """Gets a response from a language model.

    :param prompt: The prompt to feed into the language model.
    :param max_tokens: The maximal number of tokens generated by the language model.
    :return: The completion from the language model.
    """
    return f"Echoing: {prompt}, {max_tokens}"


def stream_response_from_prompt(prompt: str, max_tokens: int = 3000) -> str:
    """Streams a response from a language.

    :param prompt: The prompt to feed into the language model.
    :param max_tokens: The maximal number of tokens generated by the language model.
    :yield: The next substring of the language model's completion
    """
    yield from f"Streamed Echoing: {prompt}, {max_tokens}"


class Gemini:
    """A class representation of Gemini 1.5 Pro.

    :param model: Instance of the GenerativeModel for Gemini 1.5 Pro.
    :type model: genai.GenerativeModel
    :param temp: Sets the temperature for the language model from 0 to 2.
    :type temp: float
    :param stop_sequences: Sets the stop sequences for the language model.
    :type stop_sequences: list[str]
    """

    def __init__(self, key: str):
        genai.configure(api_key=(key))
        self.model = genai.GenerativeModel(model_name="gemini-1.5-pro")
        self.temp = 1.0
        self.stop_sequences = []

    def get_response(self, prompt: str, max_tokens: int = 3000) -> str:
        """Gets a response from a language model.

        :param prompt: The prompt to feed into the language model.
        :param max_tokens: The maximal number of tokens generated by the language model.
        :return: The completion from the language model.
        """

        response = self.model.generate_content(
            prompt,
            generation_config={
                "temperature": self.temp,
                "max_output_tokens": max_tokens,
                "stop_sequences": self.stop_sequences,
            },
        )

        return response.text

    def stream_response(
        self, prompt: str, max_tokens: int = 3000
    ) -> Generator[str, None, None]:
        """Streams a response from a language.

        :param prompt: The prompt to feed into the language model.
        :param max_tokens: The maximal number of tokens generated by the language model.
        :yield: The next substring of the language model's completion
        """

        response = self.model.generate_content(
            prompt,
            generation_config={
                "temperature": self.temp,
                "max_output_tokens": max_tokens,
                "stop_sequences": self.stop_sequences,
            },
            stream = True,
        )

        for part in response:
            yield part.text

    def set_temp(self, temp: float) -> None:
        """Sets a temperature for a language model.

        :param temp: The temperature to set for the language model.
        :return: Nothing.
        :raises ValueError: If temperature is not between 0 and 2.
        """

        if not (0 <= temp <= 2):
            raise ValueError("Temperature must be between 0 and 2.")

        self.temp = temp

    def set_stop_sequences(self, stop: list[str]) -> None:
        """Sets the stop sequence for a large language model.

        :param stop: Stop sequence to be set for the language model.
        :return: Nothing.
        :raises TypeError: If stop sequences are not in list format.
        :raises ValueError: If length of stop sequences are not between 0 and 5.
        """

        if not (isinstance(stop, list)):
            raise TypeError("Stop sequences must be a list type.")
        elif not (0 <= len(stop) <= 5):
            raise ValueError("Length of stop sequences must be between 0 and 5.")

        self.stop_sequences = stop


if __name__ == "__main__":
    main()
