"""
First, complete the skeleton functions for the language model API that exist in the 
repository by adding in logic that calls some language model API. Add any useful 
keyword arguments to the functions that you see fit, like temperature, max_tokens, 
or stop_sequences. You should make two versions: one version for production that will 
use either the Gemini API or another provider permitted by campus IT; and another version 
for testing that uses Ollama, which is a local hosting tool for language models. For Ollama,
use a small model like Gemma3n or the 1.5 Billion parameter version of Deepseek, that it
may work on a development machine without a GPU.
"""
# make functions that call the class to make get response and stream response
# take a look at api/routes.py

import os
import json
import google.generativeai as genai
import ollama
from typing import Generator, Optional, List

API_KEY = # os.environ.get("GEMINI_API_KEY", "YOUR_API_KEY_HERE")
MODE = # os.environ.get("LLM_MODE", "testing")

def main():
    client = Gemini(API_KEY)

    
def get_response_from_prompt(prompt: str, max_tokens: int = 3000) -> str:
    """Gets a response from a language model.

    :param prompt: The prompt to feed into the language model.
    :param max_tokens: The maximal number of tokens generated by the language model.
    :return: The completion from the language model.
    """
    return f"Echoing: {prompt}, {max_tokens}"


def stream_response_from_prompt(prompt: str, max_tokens: int = 3000) -> str:
    """Streams a response from a language.

    :param prompt: The prompt to feed into the language model.
    :param max_tokens: The maximal number of tokens generated by the language model.
    :yield: The next substring of the language model's completion
    """
    yield from f"Streamed Echoing: {prompt}, {max_tokens}"


class Gemini:
    """A class representation of Gemini 1.5 Pro.

    :param model: Instance of the GenerativeModel for Gemini 1.5 Pro.
    :type model: genai.GenerativeModel
    :param temp: Sets the temperature for the language model from 0 to 2.
    :type temp: float
    :param stop_sequences: Sets the stop sequences for the language model.
    :type stop_sequences: list[str]
    """

    def __init__(self, key: str, temperature: float = 0.7, stop_sequences: Optional[List[str]] = None):
        if not key or key == "YOUR_API_KEY_HERE":
            raise ValueError("Gemini API key is required for production mode.")
        genai.configure(api_key=key)
        self.model = genai.GenerativeModel(model_name="gemini-1.5-pro")
        self.temperature = temperature
        self.stop_sequences = stop_sequences if stop_sequences is not None else []
    
    def get_response(self, prompt: str, max_tokens: int = 3000) -> str:
        """Gets a single response from the Gemini model."""
        # ... (method code is unchanged)
        generation_config = {
            "temperature": self.temperature,
            "max_output_tokens": max_tokens,
            "stop_sequences": self.stop_sequences,
        }
        response = self.model.generate_content(prompt, generation_config=generation_config)
        return response.text

    def stream_response(self, prompt: str, max_tokens: int = 3000) -> Generator[str, None, None]:
        """Streams a response from the Gemini model."""
        # ... (method code is unchanged)
        generation_config = {
            "temperature": self.temperature,
            "max_output_tokens": max_tokens,
            "stop_sequences": self.stop_sequences,
        }
        response = self.model.generate_content(prompt, generation_config=generation_config, stream=True)
        for part in response:
            yield part.text

class Ollama:
    """
    A class representation for a local Ollama API that uses the official client.
    """
    def __init__(self, model: str = "gemma:2b", host: str = "http://localhost:11434", temperature: float = 0.7, stop_sequences: Optional[List[str]] = None):
        self.model = model
        self.temperature = temperature
        self.stop_sequences = stop_sequences
        # Instantiate the official client, like in your boss's example
        try:
            self.client = ollama.Client(host=host)
            # A quick check to see if the server is responsive
            self.client.list()
        except Exception:
            raise ConnectionError(f"Could not connect to Ollama at {host}. Please ensure Ollama is running.")

    def get_response(self, prompt: str, max_tokens: int = 3000) -> str:
        """Gets a single response from the Ollama model."""
        options = {
            "temperature": self.temperature,
            "num_predict": max_tokens,
            "stop": self.stop_sequences
        }
        response = self.client.generate(
            model=self.model,
            prompt=prompt,
            stream=False,
            options=options
        )
        return response.get('response', '')

    def stream_response(self, prompt: str, max_tokens: int = 3000) -> Generator[str, None, None]:
        """Streams a response from the Ollama model."""
        options = {
            "temperature": self.temperature,
            "num_predict": max_tokens,
            "stop": self.stop_sequences
        }
        stream = self.client.generate(
            model=self.model,
            prompt=prompt,
            stream=True,
            options=options
        )
        for chunk in stream:
            yield chunk.get('response', '')

# --- Wrapper Functions (Unchanged) ---

def get_llm_client(temperature: float = 0.7, stop_sequences: Optional[List[str]] = None):
    # ... (function is unchanged)
    if MODE == "production":
        return Gemini(key=API_KEY, temperature=temperature, stop_sequences=stop_sequences)
    elif MODE == "testing":
        return Ollama(temperature=temperature, stop_sequences=stop_sequences)
    else:
        raise ValueError(f"Invalid mode: {MODE}. Choose 'production' or 'testing'.")


def get_response_from_prompt(prompt: str, max_tokens: int = 3000, temperature: float = 0.7, stop_sequences: Optional[List[str]] = None) -> str:
    # ... (function is unchanged)
    client = get_llm_client(temperature, stop_sequences)
    return client.get_response(prompt, max_tokens)


def stream_response_from_prompt(prompt: str, max_tokens: int = 3000, temperature: float = 0.7, stop_sequences: Optional[List[str]] = None) -> Generator[str, None, None]:
    # ... (function is unchanged)
    client = get_llm_client(temperature, stop_sequences)
    yield from client.stream_response(prompt, max_tokens)

    def set_temp(self, temp: float) -> None:
        """Sets a temperature for a language model.

        :param temp: The temperature to set for the language model.
        :return: Nothing.
        :raises ValueError: If temperature is not between 0 and 2.
        """

        if not (0 <= temp <= 2):
            raise ValueError("Temperature must be between 0 and 2.")

        self.temp = temp

    def set_stop_sequences(self, stop: list[str]) -> None:
        """Sets the stop sequence for a large language model.

        :param stop: Stop sequence to be set for the language model.
        :return: Nothing.
        :raises TypeError: If stop sequences are not in list format.
        :raises ValueError: If length of stop sequences are not between 0 and 5.
        """

        if not (isinstance(stop, list)):
            raise TypeError("Stop sequences must be a list type.")
        elif not (0 <= len(stop) <= 5):
            raise ValueError("Length of stop sequences must be between 0 and 5.")

        self.stop_sequences = stop


if __name__ == "__main__":
    main()
